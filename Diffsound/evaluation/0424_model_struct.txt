Model DALLE(
  (content_codec): VQModel(
    (encoder): Encoder(
      (conv_in): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (3): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (4): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList(
            (0): AttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): AttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
          )
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (decoder): Decoder(
      (conv_in): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (up): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (3): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (4): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList(
            (0): AttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): AttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (2): AttnBlock(
              (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
              (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
              (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
            )
          )
          (upsample): Upsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
      (conv_out): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): DummyLoss()
    (quantize): VectorQuantizer(
      (embedding): Embedding(256, 256)
    )
    (quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (condition_codec): Tokenize for text
  	content_length: 77
  	add_start_and_end: Trues
  	with_mask: True
  (transformer): DiffusionTransformer(
    (condition_emb): CLIPTextEmbedding(
      (token_embedding): Embedding(49408, 512)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (transformer): Text2ImageTransformer(
      (content_emb): DalleMaskImageEmbedding(
        (emb): Embedding(257, 1024)
        (height_emb): Embedding(5, 1024)
        (width_emb): Embedding(53, 1024)
      )
      (blocks): Sequential(
        (0): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (ln1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn1): FullAttention(
            (key): Linear(in_features=1024, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=1024, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (attn2): CrossAttention(
            (key): Linear(in_features=512, out_features=1024, bias=True)
            (query): Linear(in_features=1024, out_features=1024, bias=True)
            (value): Linear(in_features=512, out_features=1024, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (resid_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (ln1_1): AdaLayerNorm(
            (emb): Embedding(100, 1024)
            (silu): SiLU()
            (linear): Linear(in_features=1024, out_features=2048, bias=True)
            (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)
          )
          (mlp): Sequential(
            (0): Linear(in_features=1024, out_features=4096, bias=True)
            (1): GELU2()
            (2): Linear(in_features=4096, out_features=1024, bias=True)
            (3): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (to_logits): Sequential(
        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=1024, out_features=256, bias=True)
      )
    )
  )
  (first_stage_permuter): ColumnMajor()
)
